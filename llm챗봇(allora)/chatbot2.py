import gradio as gr
from ollama import Client

# Ollama ë¡œì»¬ ì„œë²„ ì—°ê²°
client = Client(host='http://localhost:11434')

# ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
MODEL = 'qwen3:1.7b'

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ í•œêµ­ì–´ë¡œ ì´í•´í•˜ê³  ëŒ€ë‹µí•´ì£¼ì„¸ìš”."

# ëŒ€í™” ì´ë ¥ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
chat_history = [{"role": "system", "content": SYSTEM_PROMPT}]

def generate_response(user_input, history):
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ìµœì´ˆì—ë§Œ ì¶”ê°€
    if not history:
        history = [{"role": "system", "content": SYSTEM_PROMPT}]
    else:
        history = history.copy()

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    history.append({"role": "user", "content": user_input})

    # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    response = client.chat(model=MODEL, messages=history)
    answer = response['message']['content']

    # ì‘ë‹µ ì¶”ê°€
    history.append({"role": "assistant", "content": answer})

    return answer


# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ë©€í‹°í„´ ì§€ì›)
chatbot = gr.ChatInterface(
    fn=generate_response,
    title="ğŸ’¬ Qwen3 ì±—ë´‡ (ë¡œì»¬ LLM)",
    theme="soft",
    examples=["ì•ˆë…•!", "íŒŒì´ì¬ìœ¼ë¡œ ì›¹ ì„œë²„ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜", "ë„ˆëŠ” ì–´ë–¤ ëª¨ë¸ì´ì•¼?"
              ,"ì˜¤ëŠ˜ ë‚ ì”¨ ì•Œë ¤ì¤˜", "ì ì‹¬ ë©”ë‰´ ì¶”ì²œí•´ì¤˜"],
    submit_btn="ë³´ë‚´ê¸°",
    stop_btn="ì¤‘ë‹¨"
)


if __name__ == "__main__":
    chatbot.launch()